{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMRJ+sEulMTNMv3TJmdzrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathirvel-G/pyspark/blob/main/casestudyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bd2hwu8P-yR",
        "outputId": "9ef9f21d-f289-4cd3-a9de-06c28fa7fcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: An error occurred while calling o25.jdbc.\n",
            ": java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
            "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark SQL Server Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the connection properties\n",
        "url = \"jdbc:sqlserver://LAPTOP-01FKSP77:1433;databaseName=AdventureWorks2022\"\n",
        "table = \"[AdventureWorks2022].[HumanResources].[Department]\"\n",
        "properties = {\n",
        "    \"user\": \"sa\",\n",
        "    \"password\": \"1234\",\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "}\n",
        "\n",
        "# Read data from SQL Server into a PySpark DataFrame\n",
        "try:\n",
        "    df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
        "    df.show(10)  # Show the top 10 records\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySpark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNo0MYqARH-F",
        "outputId": "e215ab35-5c58-4cff-fb7e-2dbfe3d4fe36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySpark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from PySpark) (0.10.9.7)\n",
            "Building wheels for collected packages: PySpark\n",
            "  Building wheel for PySpark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PySpark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=d8d8d4afe3f7b4b9957d96aa55245663da59bc6222d552eaccfa503a1ede11f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built PySpark\n",
            "Installing collected packages: PySpark\n",
            "Successfully installed PySpark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import urllib\n",
        "\n",
        "# Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark SQL Server connection\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Connection details\n",
        "server = 'LAPTOP-01FKSP77'\n",
        "database = 'AdventureWorks2022'\n",
        "username = 'sa'\n",
        "password = '1234'\n",
        "\n",
        "# Build JDBC URL with necessary details\n",
        "jdbc_url = f\"jdbc:sqlserver://{server};databaseName={database}\"\n",
        "connection_properties = {\n",
        "    \"user\": username,\n",
        "    \"password\": password,\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "}\n",
        "\n",
        "# Query to be executed\n",
        "query = \"(SELECT TOP 10 * FROM [HumanResources].[Department]) AS dept\"\n",
        "\n",
        "# Read data from SQL Server into DataFrame using JDBC\n",
        "try:\n",
        "    df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
        "    df.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Stop the Spark session after execution\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "U00xOcsUR_J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a89fb8-8230-4655-e838-effebe65cf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: An error occurred while calling o25.jdbc.\n",
            ": java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
            "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
            "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
            "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}